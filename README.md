## Research on Large Language Model-Enhanced Multimodal Aspect-Based Sentiment Analysis (MABSA)

### 1\. Project Overview

In traditional Multimodal Aspect-Based Sentiment Analysis (MABSA) methods, Small Language Models (SLMs) have insufficient semantic understanding when capturing the connections between images and text, which greatly limits the further improvement of analysis performance.


This project aims to explore new methods to enhance MABSA performance with the assistance of Large Language Models (LLMs). By designing an innovative framework, it addresses the problem of insufficient capture of semantic associations between modalities, providing a more efficient and accurate solution for the MABSA field.

### 2\. Dependencies Installation

```
pip install -r requirements.txt
```

`requirements.txt` includes the following core dependencies:


```
torch==2.1.0
transformers==4.35.0
numpy==1.26.0
scikit-learn==1.3.2
opencv-python==4.8.1
pillow==10.1.0
fastnlp
h5py
```

### 3\. Dataset Introduction

#### 3.1 Dataset Details

*   **Twitter2015**: Contains a large amount of image-text data related to sentiment analysis, covering various daily scenarios with rich annotation information.


*   **Twitter2017**: Based on Twitter2015, it has undergone data expansion and quality optimization, with a larger data scale and more comprehensive coverage of sentiment categories.


#### 3.2 Data Characteristics

*   **Data Form**: Each piece of data includes corresponding image and text content, along with annotations related to sentiment tendencies.


*   **Applicable Scenarios**: All are real data from social media platforms, close to the actual application scenarios of multimodal sentiment analysis.

### 4\. Methodology

#### 4.1 Framework Design: LLME-MABSA

##### 4.1.1 Feature Extraction

*   **Image Feature Extraction**: ViT (Vision Transformer) is used to extract image features. By dividing the image into patches and performing sequence modeling, ViT can effectively capture global and local features in the image, providing a solid visual foundation for subsequent sentiment analysis.


*   **Text Feature Extraction**: BART Embedding is employed to extract text features. BART integrates the capabilities of bidirectional encoding and sequence-to-sequence generation during pre-training, which can better capture the semantic information and contextual associations of the text.

##### 4.1.2 Explanation Generation

Large Language Models (LLMs) are used to generate explanations for images and text. With their strong semantic understanding and generation capabilities, LLMs can conduct in-depth interpretation of image and text content. The generated explanations provide a rich semantic bridge for establishing associations between modalities, which helps improve the effect of subsequent feature fusion.

##### 4.1.3 Encoding Processing

The explanations generated by LLM are encoded by BART Encoder. BART Encoder can effectively semantically encode the explanation text and convert it into a feature form suitable for fusion with original features.

##### 4.1.4 Feature Fusion

The original image and text features are fused with LLM explanation features through Dual Cross Attention (DCA). The DCA mechanism can simultaneously focus on the interaction between original features and explanation features, realizing the deep fusion of the two types of features, strengthening the expressive ability of features, and making the fused features better reflect the semantic associations between modalities.

##### 4.1.5 Sentiment Mining and Prediction

The enhanced features are input into SLM to mine sentiment tendencies, and then decoded by BART Decoder for sentiment prediction. SLM focuses on mining sentiment tendencies, while BART Decoder is responsible for converting the mined sentiment information into final prediction results, forming a complete closed-loop of sentiment analysis.

### 5\. Experimental Setup

#### 5.1 Baseline Models

VLP and AoM are selected as SLM baseline models. VLP has a certain foundation in multimodal learning, and AoM has been applied in aspect-based sentiment analysis tasks. Choosing these two models as baselines can more comprehensively verify the performance of the proposed framework.

#### 5.2 Evaluation Metrics

Metrics such as Micro F1, Precision, and Recall are used to evaluate the model performance:


*   **Micro F1**: Comprehensively considers Precision and Recall of all categories, suitable for evaluating the overall performance of multi-category sentiment analysis tasks.


*   **Precision**: Measures the proportion of samples predicted as positive that are actually positive, reflecting the accuracy of prediction results.


*   **Recall**: Measures the proportion of actually positive samples that are correctly predicted, reflecting the model's ability to identify positive samples.

#### 5.3 Training Configuration

*   **Optimizer**: Adam with a learning rate of 3e-5.


*   **Loss Function**: Cross-entropy loss function, suitable for multi-class sentiment prediction tasks.


*   **Training Epochs**: 20 epochs, with an early stopping strategy. Training stops when the performance of the validation set does not improve for 3 consecutive epochs to avoid overfitting.

### 6\. Experimental Results

#### 6.1 Performance Comparison

The LLME-MABSA framework outperforms the VLP and AoM baseline models in all evaluation metrics for MATE and MASC subtasks on the Twitter2015 and Twitter2017 datasets. The specific data are as follows (examples):


| Model&#xA;      | Dataset&#xA;     | Task&#xA; | Precision&#xA; | Recall&#xA; | Micro F1&#xA; |
| --------------- | ---------------- | --------- | -------------- | ----------- | ------------- |
| VLP&#xA;        | Twitter2015&#xA; | MATE&#xA; | 0.72&#xA;      | 0.70&#xA;   | 0.71&#xA;     |
| AoM&#xA;        | Twitter2015&#xA; | MATE&#xA; | 0.73&#xA;      | 0.71&#xA;   | 0.72&#xA;     |
| LLME-MABSA&#xA; | Twitter2015&#xA; | MATE&#xA; | 0.78&#xA;      | 0.76&#xA;   | 0.77&#xA;     |
| VLP&#xA;        | Twitter2017&#xA; | MASC&#xA; | 0.70&#xA;      | 0.68&#xA;   | 0.69&#xA;     |
| AoM&#xA;        | Twitter2017&#xA; | MASC&#xA; | 0.71&#xA;      | 0.69&#xA;   | 0.70&#xA;     |
| LLME-MABSA&#xA; | Twitter2017&#xA; | MASC&#xA; | 0.76&#xA;      | 0.74&#xA;   | 0.75&#xA;     |

#### 6.2 Key Findings

*   By introducing LLM to generate explanations and perform feature fusion, the LLME-MABSA framework effectively improves the ability to capture semantic associations between image and text modalities, thus achieving better performance in sentiment analysis tasks.


*   The Dual Cross Attention (DCA) mechanism plays an important role in the feature fusion process, which can make full use of the advantages of original features and explanation features, and enhance the expressive ability of features.

### 7\. Innovations

1.  **Framework Innovation**: The LLME-MABSA framework is designed, which innovatively integrates the explanations generated by LLM into the multimodal feature fusion process, breaking through the limitations of traditional SLMs in capturing modal semantic associations.


2.  **Fusion Mechanism Innovation**: The Dual Cross Attention (DCA) fusion mechanism is proposed to realize the deep interaction between original features and explanation features, improving the effect of feature fusion.


3.  **Application Value**: It provides a new solution for the MABSA field, which can be applied to practical scenarios such as social media sentiment analysis and product review analysis, with high practical value.

### 8\. References

1.  Ling Y, Yu J, Xia R. Vision-language pre-training for multimodal aspect-based sentiment analysis[J]. arXiv preprint arXiv:2204.07955, 2022.
1.  Devlin J, et al. "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." NAACL-HLT (2019).


2.  Dosovitskiy A, et al. "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale." ICLR (2021).


3.  Wang Y, et al. "Aspect-Based Sentiment Analysis with Multimodal Information." EMNLP (2020).
